version: "3.9"

services:
  openwebui:
    container_name: openwebui

    ## Change to your platform example linux/arm64 or linux/amd64
    platform: linux/amd64

    ## Look at docs https://github.com/open-webui/open-webui
    image: ghcr.io/open-webui/open-webui:main

    ## Port to expose on machine
    ports:
      - 3000:3000

    ## Model Volume mount <local path>:/app/backend/data
    volumes:
      - OPENWEBUI_DATA_PATH:/app/backend/data

    ## Environment variables see docs
    environment:
      - PORT=3000
      - OPENAI_API_BASE_URL=http://localai:8080/v1
    # Us this for AMD
     #- OPENAI_API_BASE_URL=http://localai:8081/v1
    
     

    networks:
      - localai_nvidia_ai_network

    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    stdin_open: true
    tty: true
    restart: unless-stopped

volumes:
  OPENWEBUI_DATA_PATH:
    driver: local
  
networks:
  localai_nvidia_ai_network:
    external: true
