version: "3.9"

services:
  localai:
    container_name: localai_Nvidia

    ## Change to your platform example linux/arm64 or linux/amd64
    platform: linux/amd64

    ## Platform runtime
    runtime: nvidia

    ## Look at docs and quay.io registry for images
    ## v2.24.2-aio-cpu
    ## v2.24.2-aio-gpu-nvidia-cuda-12
    ## v2.24.2-aio-gpu-hipblas
    ## v2.24.2-aio-gpu-intel-f32
    image: quay.io/go-skynet/local-ai:v2.24.2-aio-gpu-nvidia-cuda-12

    ## Port to expose on machine
    ports:
      - 8080:8080

    ## Model Volume mount <local path>:/models
    volumes:
      - LOCALAI_NVIDIA_MODELS_PATH:/models

    ## Environment variables see docs
    environment:
      - DEBUG=true
      - F16=true
      - CONTEXT_SIZE=16384
      - LLAMACPP_PARALLEL=10
      - PARALLEL_REQUESTS=true
      - WATCHDOG_IDLE=true
      - WATCHDOG_BUSY=true
      - WATCHDOG_IDLE_TIMEOUT=10m
      - WATCHDOG_BUSY_TIMEOUT=20m
      - IMAGE_PATH=/models/images/
      - UPLOAD_LIMIT=2000
      - threads=16
      - MODELS_PATH=/models
      - NVIDIA_VISIBLE_DEVICES=all
      ## - API_KEYS=""

    networks:
      - ai_network

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0,1']
              capabilities: [gpu]
              
    stdin_open: true
    tty: true
    restart: unless-stopped

networks:
  ai_network:
    driver: bridge
